---
layout: post
title: "ä½¿ç”¨SAGANç”ŸæˆäºŒæ¬¡å…ƒäººç‰©å¤´åƒ(GANç”Ÿæˆå¯¹æŠ—ç½‘ç»œ)--pytorchå®ç°"
date: 2021-08-16 15:13:14
author: "BunnyRabbit"
tags: [GAN, æ·±åº¦å­¦ä¹ , PyTorch, è‡ªæ³¨æ„åŠ›æœºåˆ¶, å›¾åƒç”Ÿæˆ]
references:
  - title: "Self-Attention Generative Adversarial Networks"
    url: "https://arxiv.org/abs/1805.08318"
    author: "Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena"
    date: "2018"
    publisher: "arXiv"
  - title: "Generative Adversarial Networks"
    url: "https://arxiv.org/abs/1406.2661"
    author: "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
    date: "2014"
    publisher: "arXiv"
  - title: "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
    url: "https://arxiv.org/abs/1912.01703"
    author: "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
    date: "2019"
    publisher: "arXiv"
---

è¿™æ˜¯è®­ç»ƒ250epochå·¦å³çš„æˆæœï¼š

![SAGANç”Ÿæˆçš„äºŒæ¬¡å…ƒå¤´åƒ](/assets/images/anime-gan.png "ä½¿ç”¨SAGANç”Ÿæˆçš„äºŒæ¬¡å…ƒäººç‰©å¤´åƒ")

ä¹‹å‰çš„æ–‡ç« é‡Œé¢ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ®‹å·®ç½‘ç»œçš„å½¢å¼å®ç°ç”Ÿæˆå™¨ä¸è¾¨åˆ«å™¨ï¼Œå®ƒç†è®ºä¸Šå¯ä»¥å®ç°å¾ˆä¸é”™çš„æ•ˆæœï¼Œä½†æœ‰ä¸€ä¸ªå¾ˆè‡´å‘½çš„ç¼ºç‚¹ï¼Œå°±æ˜¯è®­ç»ƒå¤ªæ…¢ï¼Œå¾ˆéš¾è§åˆ°æˆæœã€‚

è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶åˆ¶ä½œçš„å¯¹æŠ—ç”Ÿæˆç½‘ç»œã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯æˆ‘ä»¬åœ¨æ·±åº¦å­¦ä¹ é“è·¯ä¸Šï¼Œé™¤äº†RNN,CNNä»¥å¤–ï¼Œä¸å¾—ä¸äº†è§£çš„ä¸€ç§æ¨¡å—ã€‚éå¸¸æœ‰æ„æ€ã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¿™ä¸ªæ¨¡å—ï¼Œç›¸æ¯”äºä¹‹å‰å•çº¯ä½¿ç”¨å·ç§¯ç½‘ç»œçš„GANï¼Œå®ƒæ›´åŠ èƒ½æ³¨é‡ä¸Šä¸‹æ–‡ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œåœ¨ç”Ÿæˆäººç‰©çœ¼ç›çš„æ—¶å€™ï¼Œå®ƒä¼šæ³¨æ„åˆ°é¼»å­ğŸ‘ƒï¼Œå¤´å‘ç­‰å…¶ä»–éƒ¨ä½ï¼Œä»è€Œå°†çœ¼ç›æ”¾åœ¨åˆé€‚çš„ä½ç½®ï¼Œæ€»ä¹‹ï¼Œèƒ½æ›´å¥½çš„å­¦ä¹ åˆ°æ•´ä½“ç‰¹å¾ã€‚

## ç”Ÿæˆå™¨ä»£ç 

ä»¥ä¸‹æ˜¯ç”Ÿæˆå™¨çš„ä»£ç ã€‚æ³¨æ„attn1ä¸attn2å±‚éƒ½æ˜¯æˆ‘ä»¬çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå…¶ä»–æ˜¯æˆ‘ä»¬æ‰€ç†Ÿæ‚‰çš„DCGANä¸­ä½¿ç”¨è¿‡çš„åå·ç§¯ã€‚é»˜è®¤ç”Ÿæˆ64*64åƒç´ çš„å›¾ç‰‡ï¼Œå¦‚æœæƒ³ä¿®æ”¹å›¾ç‰‡å¤§å°ï¼Œè¯·ä¿®æ”¹image_sizeï¼Œä»¥åŠï¼Œåé¢å±‚æ¬¡ä¸­çš„channelå¤§å°ä»¥åŠselfattentionçš„å‚æ•°ã€‚

```python
class Generator(nn.Module):
    def __init__(self, image_size = 64, z_dim = 100, conv_dim = 64):
        super().__init__()
        repeat_num = int(np.log2(image_size)) - 3
        mult = 2 ** repeat_num
        self.l1 = nn.Sequential(
            spectral_norm(nn.ConvTranspose2d(
                in_channels = z_dim, 
                out_channels = conv_dim * mult, 
                kernel_size = 4, 
                stride = 1, 
                padding = 0, 
                bias = False
            )),
            nn.BatchNorm2d(conv_dim * mult),
            nn.ReLU(inplace = True)
        )
        
        # è‡ªæ³¨æ„åŠ›æ¨¡å—1
        self.attn1 = SelfAttention(conv_dim * mult)
        
        # å·ç§¯å±‚åºåˆ—
        self.l2 = nn.Sequential(
            spectral_norm(nn.ConvTranspose2d(
                in_channels = conv_dim * mult, 
                out_channels = conv_dim * (mult // 2), 
                kernel_size = 4, 
                stride = 2, 
                padding = 1, 
                bias = False
            )),
            nn.BatchNorm2d(conv_dim * (mult // 2)),
            nn.ReLU(inplace = True)
        )
        
        # è‡ªæ³¨æ„åŠ›æ¨¡å—2
        self.attn2 = SelfAttention(conv_dim * (mult // 2))
        
        # åç»­å·ç§¯å±‚
        layers = []
        for i in range(repeat_num - 1):
            layers.append(
                spectral_norm(nn.ConvTranspose2d(
                    in_channels = conv_dim * (mult // (2 ** i)), 
                    out_channels = conv_dim * (mult // (2 ** (i + 1))), 
                    kernel_size = 4, 
                    stride = 2, 
                    padding = 1, 
                    bias = False
                ))
            )
            layers.append(nn.BatchNorm2d(conv_dim * (mult // (2 ** (i + 1)))))
            layers.append(nn.ReLU(inplace = True))
        
        self.l3 = nn.Sequential(*layers)
        
        # è¾“å‡ºå±‚
        self.l4 = nn.Sequential(
            nn.ConvTranspose2d(
                in_channels = conv_dim, 
                out_channels = 3, 
                kernel_size = 4, 
                stride = 2, 
                padding = 1, 
                bias = False
            ),
            nn.Tanh()
        )
    
    def forward(self, z):
        out = self.l1(z)
        out = self.attn1(out)
        out = self.l2(out)
        out = self.attn2(out)
        out = self.l3(out)
        out = self.l4(out)
        return out
```

## è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŠ¿

è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸­çš„åº”ç”¨å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. **å…¨å±€ä¸Šä¸‹æ–‡ç†è§£**ï¼šèƒ½å¤Ÿæ•è·å›¾åƒä¸­è¿œè·ç¦»åƒç´ ä¹‹é—´çš„ä¾èµ–å…³ç³»
2. **ç‰¹å¾èåˆèƒ½åŠ›**ï¼šæ›´å¥½åœ°èåˆå›¾åƒä¸­çš„ä¸åŒç‰¹å¾ï¼Œç”Ÿæˆæ›´è¿è´¯çš„å›¾åƒ
3. **ç»†èŠ‚ç”Ÿæˆ**ï¼šåœ¨ç”Ÿæˆå¤æ‚ç»“æ„ï¼ˆå¦‚äººç‰©é¢éƒ¨ç‰¹å¾ï¼‰æ—¶è¡¨ç°æ›´å‡ºè‰²
4. **è®­ç»ƒæ•ˆç‡**ï¼šç›¸æ¯”æ®‹å·®ç½‘ç»œï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œæ›´å®¹æ˜“è§åˆ°æˆæœ

## è®­ç»ƒç»“æœ

ç»è¿‡250ä¸ªepochçš„è®­ç»ƒï¼ŒSAGANèƒ½å¤Ÿç”Ÿæˆè´¨é‡è¾ƒé«˜çš„äºŒæ¬¡å…ƒäººç‰©å¤´åƒï¼Œç›¸æ¯”ä¼ ç»Ÿçš„å·ç§¯GANï¼Œç”Ÿæˆçš„å›¾åƒåœ¨æ•´ä½“ç»“æ„å’Œç»†èŠ‚ä¸Šéƒ½æœ‰æ˜æ˜¾æå‡ã€‚

## æ€»ç»“

æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨PyTorchå®ç°åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆSAGANï¼‰æ¥ç”ŸæˆäºŒæ¬¡å…ƒäººç‰©å¤´åƒã€‚é€šè¿‡å¼•å…¥è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæˆ‘ä»¬è§£å†³äº†ä¼ ç»Ÿå·ç§¯GANåœ¨ç”Ÿæˆå¤æ‚å›¾åƒæ—¶çš„å±€é™æ€§ï¼Œå®ç°äº†æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å’Œæ›´å¥½çš„ç”Ÿæˆæ•ˆæœã€‚

è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œé™¤äº†åœ¨GANä¸­çš„åº”ç”¨ï¼Œå®ƒè¿˜å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒåˆ†ç±»ç­‰é¢†åŸŸã€‚æŒæ¡è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯¹äºæ·±å…¥ç†è§£ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚